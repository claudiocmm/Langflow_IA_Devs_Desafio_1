{"id":"9e4d9295-40f2-4a97-bc27-54aeb10e9caf","data":{"nodes":[{"id":"ChatOutput-iNG6H","type":"genericNode","position":{"x":3006.4125270335903,"y":739.5197775388161},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Meu Resumo","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-iNG6H","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":317,"dragging":false,"positionAbsolute":{"x":3006.4125270335903,"y":739.5197775388161}},{"id":"File-NyWe6","type":"genericNode","position":{"x":-222,"y":300.7726628926631},"data":{"type":"File","node":{"template":{"_type":"Component","path":{"trace_as_metadata":true,"file_path":"9e4d9295-40f2-4a97-bc27-54aeb10e9caf/Prompt Engineering Guide.md","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx"],"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"path","display_name":"Path","advanced":false,"dynamic":false,"info":"Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pathlib import Path\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"silent_errors","display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool"}},"description":"A generic file loader.","icon":"file-text","base_classes":["Data"],"display_name":"File","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_file","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["path","silent_errors"],"beta":false,"edited":false},"id":"File-NyWe6","description":"A generic file loader.","display_name":"File"},"selected":false,"width":384,"height":309},{"id":"ChatOutput-EBvqZ","type":"genericNode","position":{"x":4335.289739771186,"y":797.7398002151922},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Score","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-EBvqZ","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":4335.289739771186,"y":797.7398002151922},"dragging":false},{"id":"GroupNode-D8B2o","type":"genericNode","position":{"x":3643.4257016411493,"y":530.4267304767628},"data":{"id":"GroupNode-D8B2o","type":"GroupNode","node":{"display_name":"Avaliação","documentation":"","description":"Esse bloco carrega a parte de avaliação do flow.\n\n1. Score de similaridade do resumo com o artigo original\n2. Fator de redução por número de caracteres do resumo\n\nScore Final = Similaridade * Redução","template":{"chunk_size_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1000,"name":"chunk_size","display_name":"Chunk Size","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"chunk_size"}},"client_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"client","display_name":"Client","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"client"}},"code_OpenAIEmbeddings-B5dsW":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"code"}},"default_headers_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"default_headers","display_name":"Default Headers","advanced":true,"dynamic":false,"info":"Default headers to use for the API request.","title_case":false,"type":"dict","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"default_headers"}},"default_query_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"default_query","display_name":"Default Query","advanced":true,"dynamic":false,"info":"Default query parameters to use for the API request.","title_case":false,"type":"dict","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"default_query"}},"deployment_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"deployment","display_name":"Deployment","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"deployment"}},"embedding_ctx_length_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1536,"name":"embedding_ctx_length","display_name":"Embedding Context Length","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"embedding_ctx_length"}},"max_retries_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":3,"name":"max_retries","display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"max_retries"}},"model_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"options":["text-embedding-3-small","text-embedding-3-large","text-embedding-ada-002"],"required":false,"placeholder":"","show":true,"value":"text-embedding-3-small","name":"model","display_name":"Model","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"model"}},"model_kwargs_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"model_kwargs"}},"openai_api_base_OpenAIEmbeddings-B5dsW":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_api_base"}},"openai_api_key_OpenAIEmbeddings-B5dsW":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_api_key"}},"openai_api_type_OpenAIEmbeddings-B5dsW":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_type","display_name":"OpenAI API Type","advanced":true,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_api_type"}},"openai_api_version_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_version","display_name":"OpenAI API Version","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_api_version"}},"openai_organization_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_organization","display_name":"OpenAI Organization","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_organization"}},"openai_proxy_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_proxy","display_name":"OpenAI Proxy","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"openai_proxy"}},"request_timeout_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"request_timeout","display_name":"Request Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"request_timeout"}},"show_progress_bar_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"show_progress_bar","display_name":"Show Progress Bar","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"show_progress_bar"}},"skip_empty_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"skip_empty","display_name":"Skip Empty","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"skip_empty"}},"tiktoken_enable_OpenAIEmbeddings-B5dsW":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"tiktoken_enable","display_name":"TikToken Enable","advanced":true,"dynamic":false,"info":"If False, you must have transformers installed.","title_case":false,"type":"bool","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"tiktoken_enable"}},"tiktoken_model_name_OpenAIEmbeddings-B5dsW":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tiktoken_model_name","display_name":"TikToken Model Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"OpenAIEmbeddings-j8cIx","field":"tiktoken_model_name"}},"code_CosineSimilarityComponent-kFO3J":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput, HandleInput\nfrom langflow.template import Output\nfrom langflow.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introdução**\n\nA engenharia de prompts é uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tópicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitações dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocínio aritmético. Os desenvolvedores usam engenharia de prompt para projetar técnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos são testados com `text-davinci-003` (usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configurações padrão, ou seja, `temperatura=0.7` e `top-p=1`.\n\n# **Configurações LLM**\n\nAo trabalhar com prompts, você estará interagindo com o LLM diretamente ou por meio de uma API. Você pode configurar alguns parâmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura** - Resumindo, quanto menor a `temperatura`, mais determinísticos são os resultados, no sentido de que o próximo token provável mais alto é sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando saídas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possíveis. Em termos de aplicação, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para geração de poemas ou outras tarefas criativas, pode ser benéfico aumentar o valor da temperatura.\n\n**Top_p** - Da mesma forma, com o `top_p`, uma técnica de amostragem com temperatura chamada amostragem de núcleo, você pode controlar o grau de determinismo do modelo na geração de uma resposta. Se você está procurando respostas exatas e factuais, mantenha isso baixo. Se você estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendação geral é alterar um, não ambos.\n\nAntes de começar com alguns exemplos básicos, lembre-se de que seus resultados podem variar dependendo da versão do LLM que você está usando.\n\n# **Noções Básicas de Prompt**\n\n## **Prompts Básicos**\n\nVocê pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informações que você fornece e de quão bem elaboradas são. Um prompt pode conter informações como *instrução* ou *pergunta* que você está passando para o modelo e incluir outros detalhes como *contexto*, *entradas* ou *exemplos*. Você pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos começar analisando um exemplo básico de um prompt simples:\n\n*Prompt*\n\n```\nO céu é\n```\n\n*Saída:*\n\n```\nazulO céu é azul em um dia claro. Em um dia nublado, o céu pode ser cinza ou branco.\n```\n\nComo você pode ver, o modelo de linguagem gera uma continuação de strings que fazem sentido no contexto `\"O céu é\"`. A saída pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo básico também destaca a necessidade de fornecer mais contexto ou instruções sobre o que especificamente queremos alcançar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentença:O céu é\n```\n\n*Saída:*\n\n```\ntão lindo.\n```\n\nIsto é melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa é chamada de **engenharia de prompt**.\n\nO exemplo acima é uma ilustração básica do que é possível com LLMs hoje. Os LLMs de hoje são capazes de executar todos os tipos de tarefas avançadas que variam de resumo de texto a raciocínio matemático e geração de código.\n\n## **Formatação de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padrão tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instrução>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que é padrão em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, também chamado de *prompt de tiro zero*, ou seja, você está solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstração sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes têm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questão.\n\nDado o formato padrão acima, uma técnica popular e eficaz para solicitação é chamada de *prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstrações). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA versão do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que não é necessário usar o formato QA. O formato do prompt depende da tarefa em mãos. Por exemplo, você pode executar uma tarefa de classificação simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso é incrível! // PositivoIsto é mau! // NegativoUau, esse filme foi radical! // PositivoQue espetáculo horrível! //\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que é a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstrações.\n\n# **Elementos de um prompt**\n\nÀ medida que abordamos mais e mais exemplos e aplicativos possíveis com a engenharia de prompt, você notará que existem certos elementos que compõem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instrução** - uma tarefa ou instrução específica que você deseja que o modelo execute\n\n**Contexto** - pode envolver informações externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada** - é a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de saída** - indica o tipo ou formato da saída.\n\nNem todos os componentes são necessários para um prompt e o formato depende da tarefa em questão. Abordaremos exemplos mais concretos nos próximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui estão algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo começar a criar prompts, você deve ter em mente que é realmente um processo iterativo que requer muita experimentação para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's é um bom ponto de partida.\n\nVocê pode começar com prompts simples e continuar adicionando mais elementos e contexto à medida que busca melhores resultados. O controle de versão do seu prompt ao longo do caminho é vital por esse motivo. Ao ler o guia, você verá muitos exemplos em que a especificidade, a simplicidade e a concisão geralmente lhe darão melhores resultados.\n\nQuando você tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtém melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no início.\n\n### **A instrução**\n\nVocê pode criar prompts eficazes para várias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcançar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que você também precisa experimentar muito para ver o que funciona melhor. Experimente instruções diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa específicos. Normalmente, quanto mais específico e relevante for o contexto para a tarefa que você está tentando executar, melhor. Abordaremos a importância da amostragem e da adição de mais contexto nos próximos guias.\n\nOutros recomendam que as instruções sejam colocadas no início do prompt. Também é recomendado que algum separador claro como \"###\" seja usado para separar a instrução e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instrução ###Traduza o texto abaixo para o espanhol:Texto: \"olá!\"\n```\n\n*Saída:*\n\n```\n¡Hola!\n```\n\n### **Especificidade**\n\nSeja muito específico sobre a instrução e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores serão os resultados. Isso é particularmente importante quando você tem um resultado desejado ou estilo de geração que está buscando. Não há tokens ou palavras-chave específicas que levem a melhores resultados. É mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt é muito eficaz para obter a saída desejada em formatos específicos.\n\nAo criar prompts, você também deve ter em mente o tamanho do prompt, pois há limitações em relação a quão grande ele pode ser. Pensar em quão específico e detalhado você deve ser é algo a se considerar. Incluir muitos detalhes desnecessários não é necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mãos. Isso é algo que você precisará experimentar muito. Incentivamos muita experimentação e iteração para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informações específicas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vírgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda é um mistério. “Muitas vezes temos uma caixa preta entre o cérebro e o efeito que vemos na periferia”, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. “Se queremos utilizá-lo no contexto terapêutico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Saída:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada é obtido [neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecisões**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato, é fácil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descrições imprecisas. Geralmente é melhor ser específico e direto. A analogia aqui é muito semelhante à comunicação eficaz - quanto mais direta, mais eficaz a mensagem é transmitida.\n\nPor exemplo, você pode estar interessado em aprender o conceito de engenharia de prompt. Você pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicação curta, apenas algumas frases, e não seja muito descritivo.\n```\n\nNão está claro no prompt acima quantas frases usar e qual estilo. Você ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que é muito específico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino médio.\n```\n\n### **Fazer ou não fazer?**\n\nOutra dica comum ao criar prompts é evitar dizer o que não fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui está um exemplo de um chatbot de recomendação de filme falhando exatamente no que eu não quero que ele faça por causa de como escrevi a instrução -- focando no que não fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NÃO PEÇA INTERESSES. NÃO PEÇA INFORMAÇÕES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Saída:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme você gostaria de assistir? Você prefere ação, comédia, romance ou outra coisa?\n```\n\nAqui está um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente é responsável por recomendar um filme dos principais filmes de tendências globais. Deve abster-se de perguntar aos usuários sobre suas preferências e evitar pedir informações pessoais. Se o agente não tiver um filme para recomendar, ele deve responder \"Desculpe, não foi possível encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Saída:*\n\n```\nDesculpe, não tenho nenhuma informação sobre seus interesses. No entanto, aqui está uma lista dos principais filmes de tendências globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do [artigo \"Melhores práticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seção anterior, apresentamos e fornecemos exemplos básicos de como solicitar LLMs.\n\nNesta seção, forneceremos mais exemplos de como os prompts são usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos é através de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTópicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extração de informações](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta à pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificação de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversação](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Geração de código](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocínio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padrão na geração de linguagem natural é o resumo de texto. A sumarização de texto pode incluir preferências e domínios diferentes. Na verdade, uma das aplicações mais promissoras dos modelos de linguagem é a capacidade de resumir artigos e conceitos em resumos rápidos e fáceis de ler. Vamos tentar uma tarefa básica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibióticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibióticosA:\n```\n\n*Saída:*\n\n```\nOs antibióticos são um tipo de medicamento usado para tratar infecções bacterianas. Eles funcionam matando as bactérias ou impedindo-as de se reproduzir, permitindo que o sistema imunológico do corpo combata a infecção. Os antibióticos são geralmente tomados por via oral na forma de comprimidos, cápsulas ou soluções líquidas, ou às vezes administrados por via intravenosa. Eles não são eficazes contra infecções virais e usá-los de forma inadequada pode levar à resistência aos antibióticos.\n```\n\nO \"A:\" é um formato de prompt explícito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que há um futuro esperado. Neste exemplo, não está claro como isso é útil ou não, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso é muita informação e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibióticos são um tipo de medicamento usado para tratar infecções bacterianas. Eles funcionam matando as bactérias ou impedindo-as de se reproduzir, permitindo que o sistema imunológico do corpo combata a infecção. Os antibióticos são geralmente tomados por via oral na forma de comprimidos, cápsulas ou soluções líquidas, ou às vezes administrados por via intravenosa. Eles não são eficazes contra infecções virais e usá-los de forma inadequada pode levar à resistência aos antibióticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Saída:*\n\n```\nOs antibióticos são medicamentos usados para tratar infecções bacterianas, matando as bactérias ou impedindo-as de se reproduzir, mas não são eficazes contra vírus e o uso excessivo pode levar à resistência aos antibióticos.\n```\n\nSem prestar muita atenção à precisão da saída acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o parágrafo em uma frase. Você pode ficar esperto com as instruções, mas deixaremos isso para um capítulo posterior. Sinta-se à vontade para fazer uma pausa aqui e experimentar para ver se obtém melhores resultados.\n\n---\n\n## **Extração de Informações**\n\nEmbora os modelos de linguagem sejam treinados para executar a geração de linguagem natural e tarefas relacionadas, eles também são muito capazes de realizar classificação e uma série de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui está um exemplo de um prompt que extrai informações de um determinado parágrafo.\n\n*Prompt:*\n\n```\nAs declarações de contribuição do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparação de seus manuscritos e análises. Eles também devem indicar quais LLMs foram usados. Isso alertará os editores e revisores para examinar os manuscritos com mais cuidado em busca de possíveis vieses, imprecisões e créditos de origem impróprios. Da mesma forma, os periódicos científicos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no parágrafo acima**:\n```\n\n*Saída:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no parágrafo acima é o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso já é muito útil.\n\nAté agora deve ser óbvio que você pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse é um recurso poderoso que os desenvolvedores de produtos de IA já estão usando para criar produtos e experiências poderosos.\n\nFonte do parágrafo: [ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas específicas é melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instruções, contexto, entrada e indicadores de saída para obter melhores resultados.\n\nEmbora esses componentes não sejam necessários, torna-se uma boa prática, pois quanto mais específico você for com a instrução, melhores resultados obterá. Abaixo está um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"Não tenho certeza sobre a resposta\" se não tiver certeza da resposta.Contexto: Teplizumab tem suas raízes em uma empresa farmacêutica de Nova Jersey chamada Ortho Pharmaceutical. Lá, os cientistas geraram uma versão inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molécula foi capaz de se ligar à superfície das células T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeição de órgãos após transplantes renais, tornando-se o primeiro anticorpo terapêutico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Saída:*\n\n```\nCamundongos.\n```\n\nContexto obtido da [Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificação de texto**\n\nAté agora, usamos instruções simples para executar uma tarefa. Como um engenheiro de prompt, você precisará melhorar o fornecimento de melhores instruções. Mas isso não é tudo! Você também descobrirá que, para casos de uso mais difíceis, apenas fornecer instruções não será suficiente. É aqui que você precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que você pode fornecer são `input data` ou `examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificação de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nDemos a instrução para classificar o texto e o modelo respondeu com `'Neutro'` que está correto. Não há nada de errado nisso, mas digamos que o que realmente precisamos é que o modelo dê o rótulo no formato exato que queremos. Portanto, em vez de `Neutral`, queremos que retorne `neutral`. Como alcançamos isso? Existem diferentes maneiras de fazer isso. Nós nos preocupamos com a especificidade aqui, portanto, quanto mais informações pudermos fornecer, melhores serão os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Saída:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou `neutro`, que é o rótulo específico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser específico em sua saída. Para destacar por que às vezes ser específico é importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nQual é o problema aqui?\n\n---\n\n## **Conversação**\n\nTalvez uma das coisas mais interessantes que você pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua intenção e sua identidade. Isso é particularmente útil quando você está construindo sistemas de conversação, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversação capaz de gerar respostas mais técnicas e científicas às perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instrução. Às vezes, isso é chamado de *solicitação de função*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente é técnico e científico.Humano: Olá, quem é você?AI: Saudações! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Você pode me falar sobre a criação de buracos negros?IA:\n```\n\n*Saída:*\n\n```\nClaro! Buracos negros são regiões do espaço-tempo onde a força gravitacional é tão forte que nada, nem mesmo a luz, consegue escapar dela. Eles são criados quando uma estrela muito massiva morre e seu núcleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matéria e radiação ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco técnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessíveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fáceis de entender mesmo por alunos do ensino fundamental.Humano: Olá, quem é você?AI: Saudações! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Você pode me falar sobre a criação de buracos negros?IA:\n```\n\n*Saída:*\n\n```\nClaro! Os buracos negros são criados quando uma estrela muito massiva fica sem combustível e colapsa sobre si mesma. A gravidade da estrela é tão forte que nada, nem mesmo a luz, pode escapar de sua atração. Isso cria um ponto no espaço-tempo com uma intensa força gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Você pode continuar melhorando. Tenho certeza que se você adicionar mais exemplos você pode obter resultados ainda melhores.\n\n---\n\n## **Geração de Código**\n\nUma aplicação em que os LLMs são bastante eficazes é a geração de código. O Copilot é um ótimo exemplo disso. Há um grande número de tarefas de geração de código que você pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que saúda o usuário.\n\n*Prompt:*\n\n```\n/*Pergunte ao usuário o nome dele e diga \"Olá\"*/\n```\n\n*Saída:*\n\n```\nlet name = prompt(\"Qual é o seu nome?\");console.log(`Olá, ${name}!`);\n```\n\nVocê pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os níveis. Quero mostrar a você como os LLMs podem ser poderosos com um pouco mais de esforço no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Ciência da Computação\"\n```\n\n*Saída:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Ciência da Computação');\n```\n\nIsso é muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL válida.\n\n---\n\n## **Raciocínio**\n\nTalvez uma das tarefas mais difíceis para um LLM hoje seja aquela que requer alguma forma de raciocínio. O raciocínio é uma das áreas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemáticas. Dito isso, é importante observar que os LLMs atuais lutam para executar tarefas de raciocínio, portanto, isso requer técnicas de engenharia de prompt ainda mais avançadas. Abordaremos essas técnicas avançadas no próximo guia. Por enquanto, abordaremos alguns exemplos básicos para mostrar as capacidades aritméticas.\n\n*Prompt:*\n\n```\nQuanto é 9.000 * 9.000?\n```\n\n*Saída:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difícil.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída*\n\n```\nNão, os números ímpares neste grupo somam um número ímpar: 119.\n```\n\nIsso é incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os números ímpares, some-os e indique se o resultado é par ou ímpar.\n```\n\n*Saída:*\n\n```\nNúmeros ímpares: 15, 5, 13, 7, 1Total 4141 é um número ímpar.\n```\n\nMuito melhor, certo? A propósito, tentei isso algumas vezes e o sistema às vezes falha. Se você fornecer instruções melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seção do guia.\n\nNa próxima seção, abordaremos conceitos e técnicas de engenharia de prompt ainda mais avançados para melhorar o desempenho em todas essas e em tarefas mais difíceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instruções são capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seção anterior. Aqui está um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima não fornecemos nenhum exemplo ao modelo -- esses são os recursos de tiro zero em ação.\n\nO ajuste de instrução demonstrou melhorar o aprendizado de tiro zero [Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instrução é essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instruções. Além disso, [RLHF](https://arxiv.org/abs/1706.03741) (aprendizado por reforço a partir de feedback humano) foi adotado para escalar o ajuste de instruções em que o modelo é alinhado para melhor atender às preferências humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e métodos nas próximas seções.\n\nQuando o tiro zero não funciona, é recomendável fornecer demonstrações ou exemplos no prompt que levam ao prompt de poucos tiros. Na próxima seção, demonstramos a solicitação de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notáveis de disparo zero, eles ainda ficam aquém em tarefas mais complexas ao usar a configuração de disparo zero. O prompt de poucos disparos pode ser usado como uma técnica para permitir o aprendizado no contexto, onde fornecemos demonstrações no prompt para direcionar o modelo para um melhor desempenho. As demonstrações servem de condicionamento para exemplos subsequentes onde gostaríamos que o modelo gerasse uma resposta.\n\nDe acordo com [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf) poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente [(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitação de poucos disparos por meio de um exemplo apresentado em [Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa é usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\" é um pequeno animal peludo nativo da Tanzânia. Exemplo de frase que usaa palavra whatpu é:Estávamos viajando pela África e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rápido. Exemplo de frase que usaa palavra farduddlear é:\n```\n\n*Saída:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difíceis, podemos experimentar aumentar as demonstrações (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de [Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui estão mais algumas dicas sobre demonstrações/exemplares ao fazer poucos disparos:\n\n- \"o espaço do rótulo e a distribuição do texto de entrada especificado pelas demonstrações são importantes (independentemente de os rótulos estarem corretos para entradas individuais)\"\n- o formato que você usa também desempenha um papel fundamental no desempenho, mesmo que você use apenas rótulos aleatórios, isso é muito melhor do que nenhum rótulo.\n- resultados adicionais mostram que selecionar rótulos aleatórios de uma distribuição verdadeira de rótulos (em vez de uma distribuição uniforme) também ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rótulos aleatórios (o que significa que os rótulos Negativo e Positivo são atribuídos aleatoriamente às entradas):\n\n*Prompt:*\n\n```\nIsso é incrível! // NegativoIsto é mau! // PositivoUau, esse filme foi rad! // PositivoQue espetáculo horrível! //\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rótulos tenham sido randomizados. Observe que também mantivemos o formato, o que também ajuda. Na verdade, com mais experimentação, parece que os modelos GPT mais recentes que estamos experimentando estão se tornando mais robustos até mesmo para formatos aleatórios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso é incrível!Isto é mau! NegativoUau, esse filme foi rad!PositivoQue espetáculo horrível! --\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nNão há consistência no formato acima, mas o modelo ainda previu o rótulo correto. Temos que realizar uma análise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variações de prompts.\n\n### **Limitações da solicitação de poucos disparos**\n\nO prompt padrão de poucos disparos funciona bem para muitas tarefas, mas ainda não é uma técnica perfeita, especialmente ao lidar com tarefas de raciocínio mais complexas. Vamos demonstrar por que esse é o caso. Você se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzirá o seguinte:\n\n```\nSim, os números ímpares neste grupo somam 107, que é um número par.\n```\n\nEsta não é a resposta correta, o que não apenas destaca as limitações desses sistemas, mas também a necessidade de uma engenharia imediata mais avançada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitação de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.A: A resposta é Falsa.Os números ímpares neste grupo somam um número par: 17, 10, 19, 4, 8, 12, 24.A: A resposta é Verdadeira.Os números ímpares neste grupo somam um número par: 16, 11, 14, 4, 8, 13, 24.A: A resposta é Verdadeira.Os números ímpares neste grupo somam um número par: 17, 9, 10, 12, 13, 4, 2.A: A resposta é Falsa.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nA resposta é verdadeira.\n```\n\nIsso não funcionou. Parece que a solicitação de poucos disparos não é suficiente para obter respostas confiáveis para esse tipo de problema de raciocínio. O exemplo acima fornece informações básicas sobre a tarefa. Se você olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocínio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente, [inserção de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903) foi popularizada para abordar mais aritmética complexa, senso comum e tarefas de raciocínio simbólico.\n\nNo geral, parece que fornecer exemplos é útil para resolver algumas tarefas. Quando a solicitação de disparo zero e a solicitação de poucos disparos não são suficientes, isso pode significar que tudo o que foi aprendido pelo modelo não é suficiente para se sair bem na tarefa. A partir daqui, é recomendável começar a pensar em ajustar seus modelos ou experimentar técnicas de solicitação mais avançadas. A seguir, falaremos sobre uma das técnicas populares de sugestão, chamada de sugestão em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitação de cadeia de pensamento (CoT) permite recursos de raciocínio complexos por meio de etapas intermediárias de raciocínio. Você pode combiná-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocínio antes de responder.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os números ímpares (9, 15, 1) dá 25. A resposta é Falso.Os números ímpares neste grupo somam um número par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os números ímpares (17, 19) dá 36. A resposta é Verdadeiro.Os números ímpares neste grupo somam um número par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os números ímpares (11, 13) dá 24. A resposta é Verdadeiro.Os números ímpares neste grupo somam um número par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os números ímpares (17, 9, 13) dá 39. A resposta é Falso.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nSomando todos os números ímpares (15, 5, 13, 7, 1) dá 41. A resposta é Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocínio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os números ímpares (9, 15, 1) dá 25. A resposta é Falso.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nSomando todos os números ímpares (15, 5, 13, 7, 1) dá 41. A resposta é Falso.\n```\n\nTenha em mente que os autores afirmam que esta é uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem: [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente é a ideia de [zero-shot CoT](https://arxiv.org/abs/2205.11916) (Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitação original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 maçãs. Dei 2 maçãs ao vizinho e 2 ao reparador. Então fui comprar mais 5 maçãs e comi 1. Com quantas maçãs fiquei?\n```\n\n*Saída:*\n\n```\n11 maçãs\n```\n\nA resposta está incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 maçãs. Dei 2 maçãs ao vizinho e 2 ao reparador. Então fui comprar mais 5 maçãs e comi 1. Com quantas maçãs fiquei?Vamos pensar passo a passo.\n```\n\n*Saída:*\n\n```\nPrimeiro, você começou com 10 maçãs.Você deu 2 maçãs para o vizinho e 2 para o reparador, então você tinha 6 maçãs sobrando.Então você comprou mais 5 maçãs, então agora você tinha 11 maçãs.Finalmente, você comeu 1 maçã, então ficaria com 10 maçãs.\n```\n\nÉ impressionante que esse prompt simples seja eficaz nessa tarefa. Isso é particularmente útil onde você não tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"proxy":{"id":"CosineSimilarityComponent-vmRJH","field":"code"},"display_name":"Code"},"code_ScoreCalculatorComponent-yUVde":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import DataInput, MessageTextInput\nfrom langflow.template import Output\nfrom langflow.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite máximo de caracteres\n        min_score = 0.0  # Score mínimo\n        max_score = 1.0  # Score máximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"ScoreCalculatorComponent-rWaez","field":"code"}},"code_CustomComponent-63VyB":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageInput, HandleInput\nfrom langflow.template import Output\nfrom langflow.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CustomComponent-lHL9L","field":"code"}},"input_message_CustomComponent-63VyB":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_message","display_name":"Input Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The message to pass through.","title_case":false,"type":"str","proxy":{"id":"CustomComponent-lHL9L","field":"input_message"}},"code_ParseData-M2bQq":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"ParseData-mDUEG","field":"code"}},"sep_ParseData-M2bQq":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","proxy":{"id":"ParseData-mDUEG","field":"sep"}},"template_ParseData-M2bQq":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Score Inicial: {similaridade}\nFator de Redução: {fator_reducao} (1.0 = sem redução)\nScore Final: {score_final}\n","name":"template","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","proxy":{"id":"ParseData-mDUEG","field":"template"}}},"flow":{"data":{"nodes":[{"id":"OpenAIEmbeddings-j8cIx","type":"genericNode","position":{"x":3930.7495819264814,"y":1579.261560325239},"data":{"type":"OpenAIEmbeddings","node":{"template":{"_type":"Component","chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1000,"name":"chunk_size","display_name":"Chunk Size","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int"},"client":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"client","display_name":"Client","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"default_headers":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"default_headers","display_name":"Default Headers","advanced":true,"dynamic":false,"info":"Default headers to use for the API request.","title_case":false,"type":"dict"},"default_query":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"default_query","display_name":"Default Query","advanced":true,"dynamic":false,"info":"Default query parameters to use for the API request.","title_case":false,"type":"dict"},"deployment":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"deployment","display_name":"Deployment","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"embedding_ctx_length":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1536,"name":"embedding_ctx_length","display_name":"Embedding Context Length","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int"},"max_retries":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":3,"name":"max_retries","display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int"},"model":{"trace_as_metadata":true,"options":["text-embedding-3-small","text-embedding-3-large","text-embedding-ada-002"],"required":false,"placeholder":"","show":true,"value":"text-embedding-3-small","name":"model","display_name":"Model","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"openai_api_base":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"key","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"openai_api_type":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_type","display_name":"OpenAI API Type","advanced":true,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"openai_api_version":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_version","display_name":"OpenAI API Version","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_organization":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_organization","display_name":"OpenAI Organization","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_proxy":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_proxy","display_name":"OpenAI Proxy","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"request_timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"request_timeout","display_name":"Request Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float"},"show_progress_bar":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"show_progress_bar","display_name":"Show Progress Bar","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool"},"skip_empty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"skip_empty","display_name":"Skip Empty","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool"},"tiktoken_enable":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"tiktoken_enable","display_name":"TikToken Enable","advanced":true,"dynamic":false,"info":"If False, you must have transformers installed.","title_case":false,"type":"bool"},"tiktoken_model_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tiktoken_model_name","display_name":"TikToken Model Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"}},"description":"Generate embeddings using OpenAI models.","icon":"OpenAI","base_classes":["Embeddings"],"display_name":"OpenAI Embeddings","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["default_headers","default_query","chunk_size","client","deployment","embedding_ctx_length","max_retries","model","model_kwargs","openai_api_base","openai_api_key","openai_api_type","openai_api_version","openai_organization","openai_proxy","request_timeout","show_progress_bar","skip_empty","tiktoken_model_name","tiktoken_enable"],"beta":false,"edited":false},"id":"OpenAIEmbeddings-j8cIx"},"selected":true,"width":384,"height":395,"positionAbsolute":{"x":3930.7495819264814,"y":1579.261560325239},"dragging":false},{"id":"CosineSimilarityComponent-vmRJH","type":"genericNode","position":{"x":4449.337686675583,"y":1356.3189405398239},"data":{"type":"CosineSimilarityComponent","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding Model","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"Model to generate embeddings for the texts.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput, HandleInput\nfrom langflow.template import Output\nfrom langflow.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introdução**\n\nA engenharia de prompts é uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tópicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitações dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocínio aritmético. Os desenvolvedores usam engenharia de prompt para projetar técnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos são testados com `text-davinci-003` (usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configurações padrão, ou seja, `temperatura=0.7` e `top-p=1`.\n\n# **Configurações LLM**\n\nAo trabalhar com prompts, você estará interagindo com o LLM diretamente ou por meio de uma API. Você pode configurar alguns parâmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura** - Resumindo, quanto menor a `temperatura`, mais determinísticos são os resultados, no sentido de que o próximo token provável mais alto é sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando saídas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possíveis. Em termos de aplicação, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para geração de poemas ou outras tarefas criativas, pode ser benéfico aumentar o valor da temperatura.\n\n**Top_p** - Da mesma forma, com o `top_p`, uma técnica de amostragem com temperatura chamada amostragem de núcleo, você pode controlar o grau de determinismo do modelo na geração de uma resposta. Se você está procurando respostas exatas e factuais, mantenha isso baixo. Se você estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendação geral é alterar um, não ambos.\n\nAntes de começar com alguns exemplos básicos, lembre-se de que seus resultados podem variar dependendo da versão do LLM que você está usando.\n\n# **Noções Básicas de Prompt**\n\n## **Prompts Básicos**\n\nVocê pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informações que você fornece e de quão bem elaboradas são. Um prompt pode conter informações como *instrução* ou *pergunta* que você está passando para o modelo e incluir outros detalhes como *contexto*, *entradas* ou *exemplos*. Você pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos começar analisando um exemplo básico de um prompt simples:\n\n*Prompt*\n\n```\nO céu é\n```\n\n*Saída:*\n\n```\nazulO céu é azul em um dia claro. Em um dia nublado, o céu pode ser cinza ou branco.\n```\n\nComo você pode ver, o modelo de linguagem gera uma continuação de strings que fazem sentido no contexto `\"O céu é\"`. A saída pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo básico também destaca a necessidade de fornecer mais contexto ou instruções sobre o que especificamente queremos alcançar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentença:O céu é\n```\n\n*Saída:*\n\n```\ntão lindo.\n```\n\nIsto é melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa é chamada de **engenharia de prompt**.\n\nO exemplo acima é uma ilustração básica do que é possível com LLMs hoje. Os LLMs de hoje são capazes de executar todos os tipos de tarefas avançadas que variam de resumo de texto a raciocínio matemático e geração de código.\n\n## **Formatação de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padrão tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instrução>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que é padrão em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, também chamado de *prompt de tiro zero*, ou seja, você está solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstração sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes têm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questão.\n\nDado o formato padrão acima, uma técnica popular e eficaz para solicitação é chamada de *prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstrações). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA versão do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que não é necessário usar o formato QA. O formato do prompt depende da tarefa em mãos. Por exemplo, você pode executar uma tarefa de classificação simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso é incrível! // PositivoIsto é mau! // NegativoUau, esse filme foi radical! // PositivoQue espetáculo horrível! //\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que é a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstrações.\n\n# **Elementos de um prompt**\n\nÀ medida que abordamos mais e mais exemplos e aplicativos possíveis com a engenharia de prompt, você notará que existem certos elementos que compõem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instrução** - uma tarefa ou instrução específica que você deseja que o modelo execute\n\n**Contexto** - pode envolver informações externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada** - é a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de saída** - indica o tipo ou formato da saída.\n\nNem todos os componentes são necessários para um prompt e o formato depende da tarefa em questão. Abordaremos exemplos mais concretos nos próximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui estão algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo começar a criar prompts, você deve ter em mente que é realmente um processo iterativo que requer muita experimentação para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's é um bom ponto de partida.\n\nVocê pode começar com prompts simples e continuar adicionando mais elementos e contexto à medida que busca melhores resultados. O controle de versão do seu prompt ao longo do caminho é vital por esse motivo. Ao ler o guia, você verá muitos exemplos em que a especificidade, a simplicidade e a concisão geralmente lhe darão melhores resultados.\n\nQuando você tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtém melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no início.\n\n### **A instrução**\n\nVocê pode criar prompts eficazes para várias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcançar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que você também precisa experimentar muito para ver o que funciona melhor. Experimente instruções diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa específicos. Normalmente, quanto mais específico e relevante for o contexto para a tarefa que você está tentando executar, melhor. Abordaremos a importância da amostragem e da adição de mais contexto nos próximos guias.\n\nOutros recomendam que as instruções sejam colocadas no início do prompt. Também é recomendado que algum separador claro como \"###\" seja usado para separar a instrução e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instrução ###Traduza o texto abaixo para o espanhol:Texto: \"olá!\"\n```\n\n*Saída:*\n\n```\n¡Hola!\n```\n\n### **Especificidade**\n\nSeja muito específico sobre a instrução e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores serão os resultados. Isso é particularmente importante quando você tem um resultado desejado ou estilo de geração que está buscando. Não há tokens ou palavras-chave específicas que levem a melhores resultados. É mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt é muito eficaz para obter a saída desejada em formatos específicos.\n\nAo criar prompts, você também deve ter em mente o tamanho do prompt, pois há limitações em relação a quão grande ele pode ser. Pensar em quão específico e detalhado você deve ser é algo a se considerar. Incluir muitos detalhes desnecessários não é necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mãos. Isso é algo que você precisará experimentar muito. Incentivamos muita experimentação e iteração para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informações específicas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vírgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda é um mistério. “Muitas vezes temos uma caixa preta entre o cérebro e o efeito que vemos na periferia”, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. “Se queremos utilizá-lo no contexto terapêutico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Saída:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada é obtido [neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecisões**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato, é fácil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descrições imprecisas. Geralmente é melhor ser específico e direto. A analogia aqui é muito semelhante à comunicação eficaz - quanto mais direta, mais eficaz a mensagem é transmitida.\n\nPor exemplo, você pode estar interessado em aprender o conceito de engenharia de prompt. Você pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicação curta, apenas algumas frases, e não seja muito descritivo.\n```\n\nNão está claro no prompt acima quantas frases usar e qual estilo. Você ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que é muito específico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino médio.\n```\n\n### **Fazer ou não fazer?**\n\nOutra dica comum ao criar prompts é evitar dizer o que não fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui está um exemplo de um chatbot de recomendação de filme falhando exatamente no que eu não quero que ele faça por causa de como escrevi a instrução -- focando no que não fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NÃO PEÇA INTERESSES. NÃO PEÇA INFORMAÇÕES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Saída:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme você gostaria de assistir? Você prefere ação, comédia, romance ou outra coisa?\n```\n\nAqui está um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente é responsável por recomendar um filme dos principais filmes de tendências globais. Deve abster-se de perguntar aos usuários sobre suas preferências e evitar pedir informações pessoais. Se o agente não tiver um filme para recomendar, ele deve responder \"Desculpe, não foi possível encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Saída:*\n\n```\nDesculpe, não tenho nenhuma informação sobre seus interesses. No entanto, aqui está uma lista dos principais filmes de tendências globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do [artigo \"Melhores práticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seção anterior, apresentamos e fornecemos exemplos básicos de como solicitar LLMs.\n\nNesta seção, forneceremos mais exemplos de como os prompts são usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos é através de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTópicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extração de informações](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta à pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificação de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversação](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Geração de código](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocínio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padrão na geração de linguagem natural é o resumo de texto. A sumarização de texto pode incluir preferências e domínios diferentes. Na verdade, uma das aplicações mais promissoras dos modelos de linguagem é a capacidade de resumir artigos e conceitos em resumos rápidos e fáceis de ler. Vamos tentar uma tarefa básica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibióticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibióticosA:\n```\n\n*Saída:*\n\n```\nOs antibióticos são um tipo de medicamento usado para tratar infecções bacterianas. Eles funcionam matando as bactérias ou impedindo-as de se reproduzir, permitindo que o sistema imunológico do corpo combata a infecção. Os antibióticos são geralmente tomados por via oral na forma de comprimidos, cápsulas ou soluções líquidas, ou às vezes administrados por via intravenosa. Eles não são eficazes contra infecções virais e usá-los de forma inadequada pode levar à resistência aos antibióticos.\n```\n\nO \"A:\" é um formato de prompt explícito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que há um futuro esperado. Neste exemplo, não está claro como isso é útil ou não, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso é muita informação e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibióticos são um tipo de medicamento usado para tratar infecções bacterianas. Eles funcionam matando as bactérias ou impedindo-as de se reproduzir, permitindo que o sistema imunológico do corpo combata a infecção. Os antibióticos são geralmente tomados por via oral na forma de comprimidos, cápsulas ou soluções líquidas, ou às vezes administrados por via intravenosa. Eles não são eficazes contra infecções virais e usá-los de forma inadequada pode levar à resistência aos antibióticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Saída:*\n\n```\nOs antibióticos são medicamentos usados para tratar infecções bacterianas, matando as bactérias ou impedindo-as de se reproduzir, mas não são eficazes contra vírus e o uso excessivo pode levar à resistência aos antibióticos.\n```\n\nSem prestar muita atenção à precisão da saída acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o parágrafo em uma frase. Você pode ficar esperto com as instruções, mas deixaremos isso para um capítulo posterior. Sinta-se à vontade para fazer uma pausa aqui e experimentar para ver se obtém melhores resultados.\n\n---\n\n## **Extração de Informações**\n\nEmbora os modelos de linguagem sejam treinados para executar a geração de linguagem natural e tarefas relacionadas, eles também são muito capazes de realizar classificação e uma série de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui está um exemplo de um prompt que extrai informações de um determinado parágrafo.\n\n*Prompt:*\n\n```\nAs declarações de contribuição do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparação de seus manuscritos e análises. Eles também devem indicar quais LLMs foram usados. Isso alertará os editores e revisores para examinar os manuscritos com mais cuidado em busca de possíveis vieses, imprecisões e créditos de origem impróprios. Da mesma forma, os periódicos científicos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no parágrafo acima**:\n```\n\n*Saída:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no parágrafo acima é o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso já é muito útil.\n\nAté agora deve ser óbvio que você pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse é um recurso poderoso que os desenvolvedores de produtos de IA já estão usando para criar produtos e experiências poderosos.\n\nFonte do parágrafo: [ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas específicas é melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instruções, contexto, entrada e indicadores de saída para obter melhores resultados.\n\nEmbora esses componentes não sejam necessários, torna-se uma boa prática, pois quanto mais específico você for com a instrução, melhores resultados obterá. Abaixo está um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"Não tenho certeza sobre a resposta\" se não tiver certeza da resposta.Contexto: Teplizumab tem suas raízes em uma empresa farmacêutica de Nova Jersey chamada Ortho Pharmaceutical. Lá, os cientistas geraram uma versão inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molécula foi capaz de se ligar à superfície das células T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeição de órgãos após transplantes renais, tornando-se o primeiro anticorpo terapêutico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Saída:*\n\n```\nCamundongos.\n```\n\nContexto obtido da [Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificação de texto**\n\nAté agora, usamos instruções simples para executar uma tarefa. Como um engenheiro de prompt, você precisará melhorar o fornecimento de melhores instruções. Mas isso não é tudo! Você também descobrirá que, para casos de uso mais difíceis, apenas fornecer instruções não será suficiente. É aqui que você precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que você pode fornecer são `input data` ou `examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificação de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nDemos a instrução para classificar o texto e o modelo respondeu com `'Neutro'` que está correto. Não há nada de errado nisso, mas digamos que o que realmente precisamos é que o modelo dê o rótulo no formato exato que queremos. Portanto, em vez de `Neutral`, queremos que retorne `neutral`. Como alcançamos isso? Existem diferentes maneiras de fazer isso. Nós nos preocupamos com a especificidade aqui, portanto, quanto mais informações pudermos fornecer, melhores serão os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Saída:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou `neutro`, que é o rótulo específico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser específico em sua saída. Para destacar por que às vezes ser específico é importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nQual é o problema aqui?\n\n---\n\n## **Conversação**\n\nTalvez uma das coisas mais interessantes que você pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua intenção e sua identidade. Isso é particularmente útil quando você está construindo sistemas de conversação, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversação capaz de gerar respostas mais técnicas e científicas às perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instrução. Às vezes, isso é chamado de *solicitação de função*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente é técnico e científico.Humano: Olá, quem é você?AI: Saudações! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Você pode me falar sobre a criação de buracos negros?IA:\n```\n\n*Saída:*\n\n```\nClaro! Buracos negros são regiões do espaço-tempo onde a força gravitacional é tão forte que nada, nem mesmo a luz, consegue escapar dela. Eles são criados quando uma estrela muito massiva morre e seu núcleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matéria e radiação ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco técnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessíveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fáceis de entender mesmo por alunos do ensino fundamental.Humano: Olá, quem é você?AI: Saudações! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Você pode me falar sobre a criação de buracos negros?IA:\n```\n\n*Saída:*\n\n```\nClaro! Os buracos negros são criados quando uma estrela muito massiva fica sem combustível e colapsa sobre si mesma. A gravidade da estrela é tão forte que nada, nem mesmo a luz, pode escapar de sua atração. Isso cria um ponto no espaço-tempo com uma intensa força gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Você pode continuar melhorando. Tenho certeza que se você adicionar mais exemplos você pode obter resultados ainda melhores.\n\n---\n\n## **Geração de Código**\n\nUma aplicação em que os LLMs são bastante eficazes é a geração de código. O Copilot é um ótimo exemplo disso. Há um grande número de tarefas de geração de código que você pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que saúda o usuário.\n\n*Prompt:*\n\n```\n/*Pergunte ao usuário o nome dele e diga \"Olá\"*/\n```\n\n*Saída:*\n\n```\nlet name = prompt(\"Qual é o seu nome?\");console.log(`Olá, ${name}!`);\n```\n\nVocê pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os níveis. Quero mostrar a você como os LLMs podem ser poderosos com um pouco mais de esforço no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Ciência da Computação\"\n```\n\n*Saída:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Ciência da Computação');\n```\n\nIsso é muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL válida.\n\n---\n\n## **Raciocínio**\n\nTalvez uma das tarefas mais difíceis para um LLM hoje seja aquela que requer alguma forma de raciocínio. O raciocínio é uma das áreas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemáticas. Dito isso, é importante observar que os LLMs atuais lutam para executar tarefas de raciocínio, portanto, isso requer técnicas de engenharia de prompt ainda mais avançadas. Abordaremos essas técnicas avançadas no próximo guia. Por enquanto, abordaremos alguns exemplos básicos para mostrar as capacidades aritméticas.\n\n*Prompt:*\n\n```\nQuanto é 9.000 * 9.000?\n```\n\n*Saída:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difícil.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída*\n\n```\nNão, os números ímpares neste grupo somam um número ímpar: 119.\n```\n\nIsso é incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os números ímpares, some-os e indique se o resultado é par ou ímpar.\n```\n\n*Saída:*\n\n```\nNúmeros ímpares: 15, 5, 13, 7, 1Total 4141 é um número ímpar.\n```\n\nMuito melhor, certo? A propósito, tentei isso algumas vezes e o sistema às vezes falha. Se você fornecer instruções melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seção do guia.\n\nNa próxima seção, abordaremos conceitos e técnicas de engenharia de prompt ainda mais avançados para melhorar o desempenho em todas essas e em tarefas mais difíceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instruções são capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seção anterior. Aqui está um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as férias estão boas.Sentimento:\n```\n\n*Saída:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima não fornecemos nenhum exemplo ao modelo -- esses são os recursos de tiro zero em ação.\n\nO ajuste de instrução demonstrou melhorar o aprendizado de tiro zero [Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instrução é essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instruções. Além disso, [RLHF](https://arxiv.org/abs/1706.03741) (aprendizado por reforço a partir de feedback humano) foi adotado para escalar o ajuste de instruções em que o modelo é alinhado para melhor atender às preferências humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e métodos nas próximas seções.\n\nQuando o tiro zero não funciona, é recomendável fornecer demonstrações ou exemplos no prompt que levam ao prompt de poucos tiros. Na próxima seção, demonstramos a solicitação de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notáveis de disparo zero, eles ainda ficam aquém em tarefas mais complexas ao usar a configuração de disparo zero. O prompt de poucos disparos pode ser usado como uma técnica para permitir o aprendizado no contexto, onde fornecemos demonstrações no prompt para direcionar o modelo para um melhor desempenho. As demonstrações servem de condicionamento para exemplos subsequentes onde gostaríamos que o modelo gerasse uma resposta.\n\nDe acordo com [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf) poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente [(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitação de poucos disparos por meio de um exemplo apresentado em [Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa é usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\" é um pequeno animal peludo nativo da Tanzânia. Exemplo de frase que usaa palavra whatpu é:Estávamos viajando pela África e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rápido. Exemplo de frase que usaa palavra farduddlear é:\n```\n\n*Saída:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difíceis, podemos experimentar aumentar as demonstrações (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de [Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui estão mais algumas dicas sobre demonstrações/exemplares ao fazer poucos disparos:\n\n- \"o espaço do rótulo e a distribuição do texto de entrada especificado pelas demonstrações são importantes (independentemente de os rótulos estarem corretos para entradas individuais)\"\n- o formato que você usa também desempenha um papel fundamental no desempenho, mesmo que você use apenas rótulos aleatórios, isso é muito melhor do que nenhum rótulo.\n- resultados adicionais mostram que selecionar rótulos aleatórios de uma distribuição verdadeira de rótulos (em vez de uma distribuição uniforme) também ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rótulos aleatórios (o que significa que os rótulos Negativo e Positivo são atribuídos aleatoriamente às entradas):\n\n*Prompt:*\n\n```\nIsso é incrível! // NegativoIsto é mau! // PositivoUau, esse filme foi rad! // PositivoQue espetáculo horrível! //\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rótulos tenham sido randomizados. Observe que também mantivemos o formato, o que também ajuda. Na verdade, com mais experimentação, parece que os modelos GPT mais recentes que estamos experimentando estão se tornando mais robustos até mesmo para formatos aleatórios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso é incrível!Isto é mau! NegativoUau, esse filme foi rad!PositivoQue espetáculo horrível! --\n```\n\n*Saída:*\n\n```\nNegativo\n```\n\nNão há consistência no formato acima, mas o modelo ainda previu o rótulo correto. Temos que realizar uma análise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variações de prompts.\n\n### **Limitações da solicitação de poucos disparos**\n\nO prompt padrão de poucos disparos funciona bem para muitas tarefas, mas ainda não é uma técnica perfeita, especialmente ao lidar com tarefas de raciocínio mais complexas. Vamos demonstrar por que esse é o caso. Você se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzirá o seguinte:\n\n```\nSim, os números ímpares neste grupo somam 107, que é um número par.\n```\n\nEsta não é a resposta correta, o que não apenas destaca as limitações desses sistemas, mas também a necessidade de uma engenharia imediata mais avançada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitação de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.A: A resposta é Falsa.Os números ímpares neste grupo somam um número par: 17, 10, 19, 4, 8, 12, 24.A: A resposta é Verdadeira.Os números ímpares neste grupo somam um número par: 16, 11, 14, 4, 8, 13, 24.A: A resposta é Verdadeira.Os números ímpares neste grupo somam um número par: 17, 9, 10, 12, 13, 4, 2.A: A resposta é Falsa.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nA resposta é verdadeira.\n```\n\nIsso não funcionou. Parece que a solicitação de poucos disparos não é suficiente para obter respostas confiáveis para esse tipo de problema de raciocínio. O exemplo acima fornece informações básicas sobre a tarefa. Se você olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocínio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente, [inserção de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903) foi popularizada para abordar mais aritmética complexa, senso comum e tarefas de raciocínio simbólico.\n\nNo geral, parece que fornecer exemplos é útil para resolver algumas tarefas. Quando a solicitação de disparo zero e a solicitação de poucos disparos não são suficientes, isso pode significar que tudo o que foi aprendido pelo modelo não é suficiente para se sair bem na tarefa. A partir daqui, é recomendável começar a pensar em ajustar seus modelos ou experimentar técnicas de solicitação mais avançadas. A seguir, falaremos sobre uma das técnicas populares de sugestão, chamada de sugestão em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitação de cadeia de pensamento (CoT) permite recursos de raciocínio complexos por meio de etapas intermediárias de raciocínio. Você pode combiná-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocínio antes de responder.\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os números ímpares (9, 15, 1) dá 25. A resposta é Falso.Os números ímpares neste grupo somam um número par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os números ímpares (17, 19) dá 36. A resposta é Verdadeiro.Os números ímpares neste grupo somam um número par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os números ímpares (11, 13) dá 24. A resposta é Verdadeiro.Os números ímpares neste grupo somam um número par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os números ímpares (17, 9, 13) dá 39. A resposta é Falso.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nSomando todos os números ímpares (15, 5, 13, 7, 1) dá 41. A resposta é Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocínio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs números ímpares neste grupo somam um número par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os números ímpares (9, 15, 1) dá 25. A resposta é Falso.Os números ímpares neste grupo somam um número par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Saída:*\n\n```\nSomando todos os números ímpares (15, 5, 13, 7, 1) dá 41. A resposta é Falso.\n```\n\nTenha em mente que os autores afirmam que esta é uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem: [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente é a ideia de [zero-shot CoT](https://arxiv.org/abs/2205.11916) (Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitação original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 maçãs. Dei 2 maçãs ao vizinho e 2 ao reparador. Então fui comprar mais 5 maçãs e comi 1. Com quantas maçãs fiquei?\n```\n\n*Saída:*\n\n```\n11 maçãs\n```\n\nA resposta está incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 maçãs. Dei 2 maçãs ao vizinho e 2 ao reparador. Então fui comprar mais 5 maçãs e comi 1. Com quantas maçãs fiquei?Vamos pensar passo a passo.\n```\n\n*Saída:*\n\n```\nPrimeiro, você começou com 10 maçãs.Você deu 2 maçãs para o vizinho e 2 para o reparador, então você tinha 6 maçãs sobrando.Então você comprou mais 5 maçãs, então agora você tinha 11 maçãs.Finalmente, você comeu 1 maçã, então ficaria com 10 maçãs.\n```\n\nÉ impressionante que esse prompt simples seja eficaz nessa tarefa. Isso é particularmente útil onde você não tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"text1":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"text1","display_name":"Text 1","advanced":false,"input_types":["Message"],"dynamic":false,"info":"First text input for similarity calculation.","title_case":false,"type":"str"}},"description":"Calculates cosine similarity between two texts.","icon":"cosine","base_classes":["Data"],"display_name":"Cosine Similarity","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"cosine_similarity","display_name":"Cosine Similarity","method":"calculate_cosine_similarity","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["text1","embedding"],"beta":false,"edited":true},"id":"CosineSimilarityComponent-vmRJH","description":"Calculates cosine similarity between two texts.","display_name":"Cosine Similarity"},"selected":true,"width":384,"height":357,"positionAbsolute":{"x":4449.337686675583,"y":1356.3189405398239},"dragging":false},{"id":"ScoreCalculatorComponent-rWaez","type":"genericNode","position":{"x":4939.6353751285205,"y":934.604278998075},"data":{"type":"ScoreCalculatorComponent","node":{"template":{"_type":"Component","llm_score":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"llm_score","display_name":"LLM Score","advanced":false,"input_types":["Data"],"dynamic":false,"info":"Initial LLM score.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import DataInput, MessageTextInput\nfrom langflow.template import Output\nfrom langflow.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite máximo de caracteres\n        min_score = 0.0  # Score mínimo\n        max_score = 1.0  # Score máximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"resposta":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"resposta","display_name":"Resposta","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Response text for the score calculation.","title_case":false,"type":"str"}},"description":"Calculates a score based on the initial LLM score and the length of the response.","icon":"calculator","base_classes":["Data"],"display_name":"Custom Component","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"final_score","display_name":"Final Score","method":"calculate_score","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["llm_score","resposta"],"beta":false,"edited":true},"id":"ScoreCalculatorComponent-rWaez","description":"Calculates a score based on the initial LLM score and the length of the response.","display_name":"Custom Component"},"selected":true,"width":384,"height":385,"positionAbsolute":{"x":4939.6353751285205,"y":934.604278998075},"dragging":false},{"id":"CustomComponent-lHL9L","type":"genericNode","position":{"x":3916.5450704806644,"y":991.92079956417},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageInput, HandleInput\nfrom langflow.template import Output\nfrom langflow.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_message","display_name":"Input Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The message to pass through.","title_case":false,"type":"str"}},"description":"Passes a message through without any modifications.","icon":"message","base_classes":["Message"],"display_name":"Message","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output_message","display_name":"Output Message","method":"pass_message","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_message"],"beta":false,"edited":true},"id":"CustomComponent-lHL9L","description":"Passes a message through without any modifications.","display_name":"Message"},"selected":true,"width":384,"height":337,"dragging":false,"positionAbsolute":{"x":3916.5450704806644,"y":991.92079956417}},{"id":"ParseData-mDUEG","type":"genericNode","position":{"x":5565.351914880647,"y":1233.1663723127},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Score Inicial: {similaridade}\nFator de Redução: {fator_reducao} (1.0 = sem redução)\nScore Final: {score_final}\n","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["data","template","sep"],"beta":false,"edited":false},"id":"ParseData-mDUEG"},"selected":true,"width":384,"height":385,"positionAbsolute":{"x":5565.351914880647,"y":1233.1663723127},"dragging":false}],"edges":[{"source":"OpenAIEmbeddings-j8cIx","sourceHandle":"{œdataTypeœ:œOpenAIEmbeddingsœ,œidœ:œOpenAIEmbeddings-j8cIxœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"CosineSimilarityComponent-vmRJH","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œCosineSimilarityComponent-vmRJHœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"CosineSimilarityComponent-vmRJH","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OpenAIEmbeddings","id":"OpenAIEmbeddings-j8cIx","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OpenAIEmbeddings-j8cIx{œdataTypeœ:œOpenAIEmbeddingsœ,œidœ:œOpenAIEmbeddings-j8cIxœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-CosineSimilarityComponent-vmRJH{œfieldNameœ:œembeddingœ,œidœ:œCosineSimilarityComponent-vmRJHœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","selected":true},{"source":"CustomComponent-lHL9L","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-lHL9Lœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}","target":"CosineSimilarityComponent-vmRJH","targetHandle":"{œfieldNameœ:œtext1œ,œidœ:œCosineSimilarityComponent-vmRJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"text1","id":"CosineSimilarityComponent-vmRJH","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-lHL9L","name":"output_message","output_types":["Message"]}},"id":"reactflow__edge-CustomComponent-lHL9L{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-lHL9Lœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}-CosineSimilarityComponent-vmRJH{œfieldNameœ:œtext1œ,œidœ:œCosineSimilarityComponent-vmRJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","selected":true},{"source":"CustomComponent-lHL9L","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-lHL9Lœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}","target":"ScoreCalculatorComponent-rWaez","targetHandle":"{œfieldNameœ:œrespostaœ,œidœ:œScoreCalculatorComponent-rWaezœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"resposta","id":"ScoreCalculatorComponent-rWaez","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-lHL9L","name":"output_message","output_types":["Message"]}},"id":"reactflow__edge-CustomComponent-lHL9L{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-lHL9Lœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}-ScoreCalculatorComponent-rWaez{œfieldNameœ:œrespostaœ,œidœ:œScoreCalculatorComponent-rWaezœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","selected":true},{"source":"ScoreCalculatorComponent-rWaez","sourceHandle":"{œdataTypeœ:œScoreCalculatorComponentœ,œidœ:œScoreCalculatorComponent-rWaezœ,œnameœ:œfinal_scoreœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-mDUEG","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-mDUEGœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-mDUEG","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"ScoreCalculatorComponent","id":"ScoreCalculatorComponent-rWaez","name":"final_score","output_types":["Data"]}},"id":"reactflow__edge-ScoreCalculatorComponent-rWaez{œdataTypeœ:œScoreCalculatorComponentœ,œidœ:œScoreCalculatorComponent-rWaezœ,œnameœ:œfinal_scoreœ,œoutput_typesœ:[œDataœ]}-ParseData-mDUEG{œfieldNameœ:œdataœ,œidœ:œParseData-mDUEGœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","selected":true},{"source":"CosineSimilarityComponent-vmRJH","sourceHandle":"{œdataTypeœ:œCosineSimilarityComponentœ,œidœ:œCosineSimilarityComponent-vmRJHœ,œnameœ:œcosine_similarityœ,œoutput_typesœ:[œDataœ]}","target":"ScoreCalculatorComponent-rWaez","targetHandle":"{œfieldNameœ:œllm_scoreœ,œidœ:œScoreCalculatorComponent-rWaezœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm_score","id":"ScoreCalculatorComponent-rWaez","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"CosineSimilarityComponent","id":"CosineSimilarityComponent-vmRJH","name":"cosine_similarity","output_types":["Data"]}},"id":"reactflow__edge-CosineSimilarityComponent-vmRJH{œdataTypeœ:œCosineSimilarityComponentœ,œidœ:œCosineSimilarityComponent-vmRJHœ,œnameœ:œcosine_similarityœ,œoutput_typesœ:[œDataœ]}-ScoreCalculatorComponent-rWaez{œfieldNameœ:œllm_scoreœ,œidœ:œScoreCalculatorComponent-rWaezœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","selected":true}],"viewport":{"zoom":1,"x":0,"y":0}},"is_component":false,"name":"Loving Euler","description":"","id":"L5k23"},"outputs":[{"types":["Message"],"selected":"Message","name":"ParseData-M2bQq_text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true,"hidden":false,"proxy":{"id":"ParseData-mDUEG","name":"text","nodeDisplayName":"Parse Data"}}]}},"selected":false,"width":384,"height":655,"dragging":false,"positionAbsolute":{"x":3643.4257016411493,"y":530.4267304767628}},{"id":"SplitText-0YDFG","type":"genericNode","position":{"x":435.15435637354335,"y":-35.671875},"data":{"type":"SplitText","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"data_inputs","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to split.","title_case":false,"type":"other"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"3000","name":"chunk_overlap","display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"Number of characters to overlap between chunks.","title_case":false,"type":"int","load_from_db":false},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"10000","name":"chunk_size","display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum number of characters in each chunk.","title_case":false,"type":"int","load_from_db":false},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separator":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"separator","display_name":"Separator","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The character to split on. Defaults to newline.","title_case":false,"type":"str"}},"description":"Split text into chunks based on specified criteria.","icon":"scissors-line-dashed","base_classes":["Data"],"display_name":"Split Text","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"chunks","display_name":"Chunks","method":"split_text","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["data_inputs","chunk_overlap","chunk_size","separator"],"beta":false,"edited":false},"id":"SplitText-0YDFG","description":"Split text into chunks based on specified criteria.","display_name":"Split Text"},"selected":false,"width":384,"height":545,"dragging":false},{"id":"OpenAIModel-nCRlh","type":"genericNode","position":{"x":453.14099221238143,"y":670.4448638072923},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"required":false,"placeholder":"","show":true,"value":"gpt-3.5-turbo","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"123","name":"seed","display_name":"Seed","advanced":false,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"beta":false,"edited":false},"id":"OpenAIModel-nCRlh"},"selected":true,"width":384,"height":725,"dragging":false,"positionAbsolute":{"x":453.14099221238143,"y":670.4448638072923}},{"id":"MapReduceChain-U2cEa","type":"genericNode","position":{"x":1192.2568690645487,"y":307.1126945634229},"data":{"type":"MapReduceChain","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"required":true,"placeholder":"","show":true,"value":"","name":"data_inputs","display_name":"Data Input","advanced":false,"input_types":["Data"],"dynamic":false,"info":"Documents separate by list","title_case":false,"type":"other"},"llm":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"llm","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain.chains import (\n    StuffDocumentsChain,\n    LLMChain,\n    ReduceDocumentsChain,\n    MapReduceDocumentsChain,\n)\n\nfrom langflow.base.chains.model import LCChainComponent\nfrom langflow.field_typing import Message\nfrom langflow.inputs import HandleInput, MultilineInput, BoolInput, DropdownInput\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.docstore.document import Document\n\n\n\nclass MapReduceDocumentsChainComponent(LCChainComponent):\n    display_name = \"Map Reduce Chain\"\n    description = \"Chain for question-answering querying sources from a retriever.\"\n    name = \"MapReduceChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"map_template\", display_name=\"Map Template\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        MultilineInput(\n            name=\"reduce_template\", display_name=\"Reduce Template\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Input\",\n            info=\"Documents separate by list\",\n            input_types=[\"Data\"],\n            is_list=True,\n            required=True\n        ),\n        BoolInput(\n            name=\"return_source_documents\",\n            display_name=\"Return Source Documents\",\n            value=False,\n        ),\n        # raw_input(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\")\n    ]\n    \n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"invoke_chain\"),\n    ]\n\n\n    def invoke_chain(self) -> Message:\n        \n        documents = []\n        for _input in self.data_inputs:\n            documents.append(Document(page_content=_input.text))\n\n        \n        # print(documents)\n        \n        # Map\n        map_prompt = PromptTemplate.from_template(self.map_template)\n        map_chain = LLMChain(llm=self.llm, prompt=map_prompt)\n        # Reduce\n        reduce_prompt = PromptTemplate.from_template(self.reduce_template)\n        reduce_chain = LLMChain(llm=self.llm, prompt=reduce_prompt)\n        \n        # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n        combine_documents_chain = StuffDocumentsChain(\n            llm_chain=reduce_chain, document_variable_name=\"docs\"\n        )\n        \n        reduce_documents_chain = ReduceDocumentsChain(\n            # This is final chain that is called.\n            combine_documents_chain=combine_documents_chain,\n            # If documents exceed context for `StuffDocumentsChain`\n            collapse_documents_chain=combine_documents_chain,\n            # The maximum number of tokens to group documents into.\n            token_max=4000,\n        )\n        \n        # Combine documents by mapping a chain over them, then combining results\n        map_reduce_chain = MapReduceDocumentsChain(\n            llm_chain=map_chain,\n            reduce_documents_chain=reduce_documents_chain,\n            document_variable_name=\"docs\",\n            return_intermediate_steps=True\n        )\n        \n\n        result = map_reduce_chain.invoke(documents)\n        result_str = result['output_text']\n        print(result['intermediate_steps'])\n        # source_docs = self.to_data(result.get(\"input_documents\", []))\n        # if self.return_source_documents and len(source_docs):\n        #     references_str = self.create_references_from_data(source_docs)\n        #     result_str = \"\\n\".join([result_str, references_str])\n        # # put the entire result to debug history, query and content\n        # self.status = {**result, \"source_documents\": result['input_documents'], \"output\": result_str}\n\n        return result_str\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"map_template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"Você é um especialista em análise de textos. \nLeia o texto a seguir e forneça um resumo conciso em markdown dos seguintes topicos:\n\nIntroducao\nDicas gerais\nExemplos de prompts\nZero-Shot\nFew-Shot\nCadeia de pensamento Prompt\n\n**Texto**\n{docs}","name":"map_template","display_name":"Map Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The input value to pass to the chain.","title_case":false,"type":"str"},"reduce_template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"pergunta:\n\n```Aja como um especialista em análise de textos. \nConsidere que os documentos sao resumos de artigo e gere um resumo final.\n\n**Requirements:**\nO resumo deve ter no MAXIMO 80 palavras, ser conciso e retornar um markdown com os seguintes topicos:\n\nIntrodução\nDicas gerais\nExemplos de práticas\nUso de plantas nativas\nUso de plantas perenes\nCiclos de cultivo\n\n**Documentos:**\nIntrodução: A jardinagem sustentável é uma abordagem ecológica que visa criar espaços verdes harmoniosos e amigos do meio ambiente. Este método não só embeleza o ambiente, mas também contribui para a conservação dos recursos naturais e para a promoção da biodiversidade local.\n\nDicas gerais: Comece com plantas nativas, pois elas estão bem adaptadas ao clima e solo locais, necessitando de menos água e cuidados. Gradualmente, vá incorporando variedades que requerem pouca água e manutenção, para criar um jardim que seja sustentável a longo prazo.\n\nExemplos de práticas: Utilize compostagem caseira para enriquecer o solo com nutrientes de forma natural e sustentável. A irrigação por gotejamento é outra prática eficaz, pois reduz o desperdício de água e garante que as plantas recebam a quantidade adequada de hidratação.\n\nUso de plantas nativas: Escolha plantas que já estão adaptadas ao clima local. Isso minimiza a necessidade de recursos adicionais, como água e fertilizantes, e promove a biodiversidade, já que essas plantas suportam melhor as condições locais e atraem a fauna nativa.\n\nUso de plantas perenes: Opte por plantas que crescem ao longo do ano sem a necessidade de replantio frequente. As plantas perenes são uma excelente escolha para um jardim sustentável, pois reduzem o trabalho de manutenção e a necessidade de novos recursos a cada estação.\n\nCiclos de cultivo: Planeje seu jardim considerando as estações do ano para manter a biodiversidade e a saúde do solo. Ao diversificar as plantas e sincronizar seus ciclos de vida com as mudanças sazonais, você promove um ecossistema equilibrado e sustentável no seu jardim.\n\n**Resumo:**```\n\nresposta:\nIntrodução: A jardinagem sustentável cria espaços verdes ecológicos e preserva recursos naturais.\n\nDicas gerais: Comece com plantas nativas e variedades de baixa manutenção.\n\nPráticas: Utilize compostagem caseira e irrigação por gotejamento.\n\nPlantas nativas: Escolha plantas adaptadas ao clima local.\n\nPlantas perenes: Opte por plantas sem replantio frequente.\n\nCiclos de cultivo: Planeje conforme as estações para manter biodiversidade e saúde do solo.\n\npergunta:\n```Aja como um especialista em análise de textos. \nConsidere que os documentos sao resumos de artigo e gere um resumo final.\n\n**Requirements:**\nO resumo deve ter no MAXIMO 80 palavras, ser conciso e retornar um markdown com os seguintes topicos:\n\nIntroducao\nDicas gerais\nExemplos de prompts\nZero-Shot\nFew-Shot\nCadeia de pensamento Prompt\n\n**Documentos:**\n{docs}\n\n**Resumo:**```\n\nresposta:","name":"reduce_template","display_name":"Reduce Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The input value to pass to the chain.","title_case":false,"type":"str"},"return_source_documents":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"return_source_documents","display_name":"Return Source Documents","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"bool"}},"description":"Map Reduce to create a better text summary for big contexts","base_classes":["Message"],"display_name":"MapReduceDocumentsChain","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"invoke_chain","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["map_template","reduce_template","llm","data_inputs","return_source_documents"],"beta":false,"edited":true},"id":"MapReduceChain-U2cEa","description":"Chain for question-answering querying sources from a retriever.","display_name":"MapReduceDocumentsChain"},"selected":false,"width":384,"height":627,"positionAbsolute":{"x":1192.2568690645487,"y":307.1126945634229},"dragging":false},{"id":"Prompt-sGAqd","type":"genericNode","position":{"x":1827.3533788156328,"y":594.754932917471},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"pergunta:\n```Aja como um especialista em análise de textos. \nConsidere o contexto e faca um resumo conciso e retorne em markdown\n\n**Requirements:**\nO resumo deve ter no MAXIMO 55 palavras, ser conciso e retornar um markdown com os seguintes topicos:\n\nIntroducao\nDicas gerais\nPráticas\nBrainstorming\nRevisão\nFeedback\n\n**Contexto:**\nIntrodução: A escrita criativa é uma forma de expressão que envolve a criação de histórias e personagens únicos, permitindo explorar a imaginação e transmitir emoções e ideias de maneira envolvente.\n\nDicas gerais: Comece com ideias simples e, gradualmente, desenvolva a complexidade da trama, adicionando camadas de profundidade e nuances aos personagens.\n\nPráticas: Use descrições detalhadas para pintar um quadro vívido na mente do leitor e diálogos autênticos para dar vida aos personagens.\n\nBrainstorming: Faça listas de ideias e explore diferentes enredos e possibilidades, permitindo que sua criatividade flua livremente.\n\nRevisão: Edite o texto várias vezes, focando em melhorar a clareza, o ritmo e o impacto emocional da história.\n\nFeedback: Compartilhe seus escritos com outras pessoas para obter diferentes perspectivas, críticas construtivas e sugestões que podem ajudar a aprimorar suas habilidades.\n\n**Resumo:**```\n\nresposta:\n**Introdução** A escrita criativa envolve criar histórias e personagens únicos, explorando a imaginação e transmitindo emoções.\n**Dicas gerais** Comece com ideias simples e desenvolva a complexidade da trama.\n**Práticas** Use descrições detalhadas e diálogos autênticos.\n**Brainstorming** Explore diferentes enredos e ideias.\n**Revisão** Edite para clareza e impacto.\n**Feedback** Compartilhe para obter críticas construtivas.\n\npergunta:\n```Aja como um especialista em análise de textos. \nConsidere o contexto e faca um resumo conciso e retorne em markdown\n\n**Requirements:**\nO resumo deve ter no MAXIMO 5 palavras, ser conciso e retornar um markdown com os seguintes topicos:\n\nIntroducao\nDicas gerais\nExemplos de práticas\nUso de plantas nativas\nUso de plantas perenes\nCiclos de cultivo\n\n**Contexto:**\nIntrodução: A jardinagem sustentável é uma abordagem ecológica que visa criar espaços verdes harmoniosos e amigos do meio ambiente. Este método não só embeleza o ambiente, mas também contribui para a conservação dos recursos naturais e para a promoção da biodiversidade local.\n\nDicas gerais: Comece com plantas nativas, pois elas estão bem adaptadas ao clima e solo locais, necessitando de menos água e cuidados. Gradualmente, vá incorporando variedades que requerem pouca água e manutenção, para criar um jardim que seja sustentável a longo prazo.\n\nExemplos de práticas: Utilize compostagem caseira para enriquecer o solo com nutrientes de forma natural e sustentável. A irrigação por gotejamento é outra prática eficaz, pois reduz o desperdício de água e garante que as plantas recebam a quantidade adequada de hidratação.\n\nUso de plantas nativas: Escolha plantas que já estão adaptadas ao clima local. Isso minimiza a necessidade de recursos adicionais, como água e fertilizantes, e promove a biodiversidade, já que essas plantas suportam melhor as condições locais e atraem a fauna nativa.\n\nUso de plantas perenes: Opte por plantas que crescem ao longo do ano sem a necessidade de replantio frequente. As plantas perenes são uma excelente escolha para um jardim sustentável, pois reduzem o trabalho de manutenção e a necessidade de novos recursos a cada estação.\n\nCiclos de cultivo: Planeje seu jardim considerando as estações do ano para manter a biodiversidade e a saúde do solo. Ao diversificar as plantas e sincronizar seus ciclos de vida com as mudanças sazonais, você promove um ecossistema equilibrado e sustentável no seu jardim.\n\n**Resumo:**```\n\nresposta:\n**Introdução** A jardinagem sustentável cria espaços verdes ecológicos e preserva recursos naturais.\n**Dicas gerais** Comece com plantas nativas e variedades de baixa manutenção.\n**Práticas** Utilize compostagem caseira e irrigação por gotejamento.\n**Plantas nativas** Escolha plantas adaptadas ao clima local.\n**Plantas perenes** Opte por plantas sem replantio frequente.\n**Ciclos de cultivo** Planeje conforme as estações para manter biodiversidade e saúde do solo.\n\n\npergunta:\n```Aja como um especialista em análise de textos. \nConsidere o contexto e faca um resumo conciso e retorne em markdown\n\n**Requirements:**\nO resumo deve ter no MAXIMO 55 palavras, ser conciso e retornar um markdown com os seguintes topicos:\n\nIntroducao\nDicas gerais\nExemplos de prompts\nZero-Shot\nFew-Shot\nCadeia de pensamento Prompt\n\n**Contexto:**\n{context}\n\n**Resumo:**```","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt de resumo Final","documentation":"","custom_fields":{"template":["context"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-sGAqd","description":"Create a prompt template with dynamic variables.","display_name":"Prompt de resumo Final"},"selected":false,"width":384,"height":431,"positionAbsolute":{"x":1827.3533788156328,"y":594.754932917471},"dragging":false},{"id":"OpenAIModel-4cCl1","type":"genericNode","position":{"x":2392.142899124554,"y":512.9606504686772},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"required":false,"placeholder":"","show":true,"value":"gpt-3.5-turbo","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"123","name":"seed","display_name":"Seed","advanced":false,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"beta":false,"edited":false},"id":"OpenAIModel-4cCl1"},"selected":false,"width":384,"height":725,"positionAbsolute":{"x":2392.142899124554,"y":512.9606504686772},"dragging":false}],"edges":[{"source":"GroupNode-D8B2o","target":"ChatOutput-EBvqZ","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-D8B2oœ,œnameœ:œParseData-M2bQq_textœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-EBvqZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-GroupNode-D8B2o{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-D8B2oœ,œnameœ:œParseData-M2bQq_textœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-EBvqZ{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-EBvqZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-EBvqZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"GroupNode-D8B2o","name":"ParseData-M2bQq_text","output_types":["Message"]}},"selected":false,"className":""},{"source":"ChatOutput-iNG6H","target":"GroupNode-D8B2o","sourceHandle":"{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-iNG6Hœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_message_CustomComponent-63VyBœ,œidœ:œGroupNode-D8B2oœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_messageœ,œidœ:œCustomComponent-lHL9Lœ},œtypeœ:œstrœ}","id":"reactflow__edge-ChatOutput-iNG6H{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-iNG6Hœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-GroupNode-D8B2o{œfieldNameœ:œinput_message_CustomComponent-63VyBœ,œidœ:œGroupNode-D8B2oœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_messageœ,œidœ:œCustomComponent-lqc3fœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_message_CustomComponent-63VyB","id":"GroupNode-D8B2o","inputTypes":["Message"],"proxy":{"field":"input_message","id":"CustomComponent-lHL9L"},"type":"str"},"sourceHandle":{"dataType":"ChatOutput","id":"ChatOutput-iNG6H","name":"message","output_types":["Message"]}},"selected":false,"className":""},{"source":"File-NyWe6","target":"SplitText-0YDFG","sourceHandle":"{œdataTypeœ:œFileœ,œidœ:œFile-NyWe6œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-0YDFGœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-File-NyWe6{œdataTypeœ:œFileœ,œidœ:œFile-NyWe6œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-0YDFG{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-0YDFGœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"SplitText-0YDFG","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"File","id":"File-NyWe6","name":"data","output_types":["Data"]}},"selected":false,"className":""},{"source":"SplitText-0YDFG","sourceHandle":"{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-0YDFGœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}","target":"MapReduceChain-U2cEa","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œMapReduceChain-U2cEaœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"MapReduceChain-U2cEa","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SplitText","id":"SplitText-0YDFG","name":"chunks","output_types":["Data"]}},"id":"reactflow__edge-SplitText-0YDFG{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-0YDFGœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-MapReduceChain-U2cEa{œfieldNameœ:œdata_inputsœ,œidœ:œMapReduceChain-U2cEaœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":""},{"source":"OpenAIModel-nCRlh","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-nCRlhœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"MapReduceChain-U2cEa","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œMapReduceChain-U2cEaœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"MapReduceChain-U2cEa","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-nCRlh","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-OpenAIModel-nCRlh{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-nCRlhœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-MapReduceChain-U2cEa{œfieldNameœ:œllmœ,œidœ:œMapReduceChain-U2cEaœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","className":""},{"source":"MapReduceChain-U2cEa","sourceHandle":"{œdataTypeœ:œMapReduceChainœ,œidœ:œMapReduceChain-U2cEaœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-sGAqd","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-sGAqdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-sGAqd","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"MapReduceChain","id":"MapReduceChain-U2cEa","name":"text","output_types":["Message"]}},"id":"reactflow__edge-MapReduceChain-U2cEa{œdataTypeœ:œMapReduceChainœ,œidœ:œMapReduceChain-U2cEaœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-sGAqd{œfieldNameœ:œcontextœ,œidœ:œPrompt-sGAqdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-sGAqd","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-sGAqdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-4cCl1","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4cCl1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-4cCl1","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-sGAqd","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-sGAqd{œdataTypeœ:œPromptœ,œidœ:œPrompt-sGAqdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-4cCl1{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4cCl1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"OpenAIModel-4cCl1","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4cCl1œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-iNG6H","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-iNG6Hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-iNG6H","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-4cCl1","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-4cCl1{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4cCl1œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-iNG6H{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-iNG6Hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""}],"viewport":{"x":365.1355888419282,"y":73.16651472326157,"zoom":0.49999999999999994}},"description":"Utilizei Map Reduce para trazer resumos consistentes do documento completo e depois utilizo um few shot para forçar uma diminuição do tamanho do resumo final, mantendo a similaridade","name":"Desafio 1 - Claudio","last_tested_version":"1.0.13","endpoint_name":null,"is_component":false}